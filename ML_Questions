__author__ = "Mahdi Moslemi"
__email__ = "moslemi.mahdi@gmail.com"


### Example of Machine Learning Questions ###


1) A Jupyter cell that contains code needs to have its type set to Code to be able to run it.

2) The series object series_obj has 5 items with the index labels 'row 1' through 'row 5', and the integer 
indexes 0 through 4 respectively. The command series_obj['row 3'] is equivalent to series_obj[2] .

3) In the dropna() DataFrame method, what will the argument axis=1 cause ... 
   ANSWERE: drop columns that contain missing values, instead of rows
    
4) You call the method duplicated() on an object and get a True value for row 8. This means that this row is 
   unique and has no duplicates.  
   ANSWERE: FALSE
   
5) What should you consider when appending a DataFrame to itself?
   Indexes will be duplicated and therefore inconsistent.
   
6) Pie charts represent data attribute values using a circle and slices that comprise it. A whole and entire set of categorical
 data is represented by the completely circle, and proportions of observations that fall into each category are represented by
 proportionate pie slices
 
 
 7) Pearson correlation assumption :::
 
    1_ Normally disturbuted
    2_ Linearly related
    3_ Continues numeric variable

8) Binomial variable:
   the variables which are just 1 or 0, so it is nit continues
   
9) Multinomial variable
   the variable which contain multi variables but it is not countinues

10) Spearmans rank assumption :::
    
    1_ your variables are ordinal (in other words numeric but able to be ranked like a categorical variable)
    2_ your variables are related non linearly
    3_ your data is non normally distributed
    
11) Chi Squre test :::

    1_ if P < 0.05 reject null hypothesis and consider the variables are correlated
    2_ if P > 0.05 reject null hypothesis and consider the variables are independent
    

 12) Why it is important to scale the data
     1_ to prevent different magnitute along the data cause misleading statics or errornous
     2_ to prepare data for machine learning
     
 12_1) Normalization
 putting all the observation on a relative scale between the value of 0 and 1
 $$ so it is like considering the value/ some of all the observation in variable $$
 
 
 12_2) Standardization
 rescaling data so it has a zero mean and unit variance
 
 
  ##### Vocabulary in Machine Learning #####
 
 Features:   term variable, coulmns, fields, and attribute.
 Instance:   terms row observation, data point, value and cases
 Data    :   terms predictor or set of peredictor variable
 Train model
 Test model
 
 Supervised method:  make prediction from labeled data
 Unsupervised method: make prediction from unlabeled data
 
 
 
 ### Factore Analysis Assumption ###
 
 Features are metric
 Features are continous or ordinal
 There is (r > 0.3) correlation between the features in your dataset
 you have >100 observation and >5 observations per feature
 your sample is homogenous
 
 
 
 ### Singular Value Decomposition (SVD) ###
 
 A linear Algebra method that decomposes a matrix into three resultant matrices to reduce information 
 redundancy and noise.
 
 Most commonly used for principal component analysy
 
 
 ## Explained variace ration
 
 the explained variace ratio tells us how much information is compressed into the first few components
 The goal for using PCA (principle component argument) method is to remove the outliers and data which are not necessary....
 PCA shows how much information each row has.
 
 
   
 ### finding OUTLIERS in the data
 
 for this purpose we can make a box plot to see the outliers or, do the calculation as follow:
 IQR = value 75% - value 25%
 P = 1.5 * IQR
 value 25% - P = new_min
 value 75% + P = new_max
 Then we compar min and max with new_min and new_max to figure out we have outlier or not.
 
 
 ### Multivariet Outlier detection
 
 we use this method to detect outliers between multi variables. these method are such as box_plot or scatter_plot
 
 
 ### DBSCAN

The parameters should result in outliers making up less than 5% of the data.
the min_sample parameter should start with a low value.
eps value should be small like 0.1


### K-Mean model ###
deponds on:
the nummber of cluster centers peresent
Nearest mean value (measured in euclidian distance between observation)

K mean use cases:
1_market price and cost modeling
2_customer segmentation
3_insurance fraud detection
4_Hedge fund classification

$$ Important notes on K-mean model:
1_  Always scale data before fitting the model
2_  Look at the scatter plote or a data table to estimate the number of centroids or cluster center, to set the k parameter in the model

$$ Precesion ::: a measure of model releavancy  
$$ Recall    ::: a measure of model completeness




### Hierarchical analysing ###

Hospital resource management
Bussiness process managmenet
customre segment
social analysis network

$$ Distance metric    ::: ecluidan , manhatan, cosin 
$$ Linkage parameter  ::: ward, comlet, average




#### KNN MODEL ####

Stock price prediction
Recommendation system
credit risk analysis
predictive modeling

KNN on larg data takes a lot of time

we use it usually when the data has little noise
dataset is labeled
dataset only contain relevent features
dataset has distinguishable subgroups

$$$$ ( High precisios + Low recall )    means few results were returned but mot of them was correct. 



##### Network Analysis #####

Social media marketing strategy
infrastructure system design
Financial risk management
Public health management


NETWORK:  A body of connected data thats evaluated during graph analysis
Graph  :  A data visualisation schematic dipicting the data that comprises a network

Graph size : number of edges in a graph
Graph order : number of nodes in a graph
Degree : The number of edges connected with a vertex with loops, connected 2 ice.


#### Linear Regression ####

All variables are continous numeric, but not categorical
Data is free of missing values or outlier
there is a linear relation between predictor and predictant
All predictor are independent of each others
Reseduals(aka prediction errors) are normally distributed



#### Logistic Regression Assumption ####

Data is free of missing value
The predicted variable is binary or categoraly encoded(order value)
All predictors are independent of eachother
we should atleat usually have 50 observation


#### Naive Bayes Classifier #####
predict the liklihood that an event occure or not

$$ we have 3 types of Naive Bayes method:   1) Multinomial    2) Bernouli   3) Gaussian
 
 1_ Multinomial :: is good when your feature (cathegorical ,or continues) describe frequency count(word count)
 
 2_ Bernouli    :: good for making prediction from binary features
 
 3_ Gaussian    :: good for making predictions from normally distributed features
 
 
 
 $$ Naive base use case :: 1) spam detection   2) Customre classification   3)Credit risk prediction   4) Health risk prediction 
 
 $$ Naive Base assumption :: 
 
 1) Predictors are independent of each others.  
 2) A perior assumption : this is a assumption hat the past conditions are still true. and we make assumtion with historical values
    ,and we get incorrect results if the present circumstance has changed. (like what we have in regression analysis)


#### PLOTLY ATRIBUTES #####

1) Traces : these are objects that describe a single variable of data in a graph, like heatmap or scatter
2) Layouts: you use these attributes to set layouts elements for your plot . for example, the title, x-axis or annotation.



  




